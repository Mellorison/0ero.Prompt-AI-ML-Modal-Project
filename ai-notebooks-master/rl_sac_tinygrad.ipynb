{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "from tinygrad.tensor import Tensor\n",
    "from tinygrad.nn import Linear\n",
    "from tinygrad.nn.optim import Adam\n",
    "from tinygrad.nn.state import get_parameters\n",
    "from tqdm import trange\n",
    "import gym\n",
    "\n",
    "# critic loss from:\n",
    "# https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/master/agents/actor_critic_agents/SAC_Discrete.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92966a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  env.close()\n",
    "except Exception:\n",
    "  pass\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "num_inputs, num_actions = env.observation_space.shape[0], env.action_space.n\n",
    "\n",
    "# global memory from the VPG/SAC learning\n",
    "memory = []\n",
    "Tensor.no_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random helpers\n",
    "def to_one_hot(x,n):\n",
    "  ret = np.zeros((len(x), n), dtype=np.float32)\n",
    "  ret[range(len(x)), x] = 1\n",
    "  return ret\n",
    "\n",
    "def reward_to_go(rews):\n",
    "  n = len(rews)\n",
    "  rtgs = np.zeros_like(rews)\n",
    "  for i in reversed(range(n)):\n",
    "    rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "  return rtgs\n",
    "\n",
    "# global params\n",
    "BS = 256       # note: VPG is always one episode, this is for Q-learning and SAC\n",
    "ALPHA = 0.2    # entropy\n",
    "GAMMA = 0.99   # discount (converges to Q=100 with 1 reward each time)\n",
    "TAU = 0.005    # update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e874588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor\n",
    "class Policy:\n",
    "  def __init__(self, num_inputs, num_actions):\n",
    "    self.w1 = Linear(num_inputs, 32)\n",
    "    self.w2 = Linear(32, num_actions)\n",
    "  def __call__(self, x:Tensor):\n",
    "    return x.sequential([self.w1, Tensor.tanh, self.w2])\n",
    "  def sample(self, obs):\n",
    "    obs = obs if isinstance(obs, Tensor) else Tensor(obs)\n",
    "    act_distribution = self(obs).softmax()\n",
    "    act_distribution_numpy = act_distribution.numpy()\n",
    "    if len(act_distribution_numpy.shape) == 2:\n",
    "      act = [np.random.choice(2, p=act_distribution_numpy[i]) for i in range(act_distribution_numpy.shape[0])]\n",
    "    else:\n",
    "      act = np.random.choice(2, p=act_distribution_numpy)\n",
    "    return act, act_distribution\n",
    "\n",
    "policy = Policy(num_inputs, num_actions)\n",
    "policy_opt = Adam(get_parameters(policy), lr=1e-2)\n",
    "env_steps = []\n",
    "env_losses = []\n",
    "ep_lens, ep_rews, ep_loss = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic\n",
    "class QNetwork:\n",
    "  def __init__(self, num_inputs, num_actions):\n",
    "    self.w1 = Linear(num_inputs+num_actions, 32)\n",
    "    self.w2 = Linear(32, 32)\n",
    "    self.w3 = Linear(32, 1)\n",
    "  def __call__(self, x:Tensor):\n",
    "    return x.sequential([self.w1, Tensor.relu, self.w2, Tensor.relu, self.w3])\n",
    "\n",
    "# init q network\n",
    "q1 = QNetwork(num_inputs, num_actions)\n",
    "q2 = QNetwork(num_inputs, num_actions)\n",
    "q1s = QNetwork(num_inputs, num_actions)\n",
    "q2s = QNetwork(num_inputs, num_actions)\n",
    "\n",
    "hard_params = get_parameters(q1) + get_parameters(q2)\n",
    "soft_params = get_parameters(q1s) + get_parameters(q2s)\n",
    "\n",
    "# NOTE: this is a hack to give them the same RAND\n",
    "# TODO: how do you really copy a Tensor in tinygrad?\n",
    "for x,y in zip(hard_params, soft_params): y.lazydata.op = x.lazydata.op\n",
    "\n",
    "q_opt = Adam(get_parameters(q1) + get_parameters(q2), lr=1e-2)\n",
    "q_losses = []\n",
    "\n",
    "def learn_q_function(samples):\n",
    "  s = [x[0] for x in samples]\n",
    "  a = [x[1] for x in samples]\n",
    "  sa = Tensor(s).cat(Tensor(to_one_hot(a, 2)), dim=1)\n",
    "  r = [x[2] for x in samples]\n",
    "  mask = [int(not x[3]) for x in samples]   # if it's the last step, mask the recursive Q\n",
    "  ns = [x[4] for x in samples]\n",
    "\n",
    "  na, _ = policy.sample(ns)\n",
    "  nsa = Tensor(ns).cat(Tensor(to_one_hot(na, 2)), dim=1)\n",
    "  q1_next, q2_next = q1s(nsa), q2s(nsa)  # max over both actions?\n",
    "  next_value = Tensor.cat(q1_next, q2_next, dim=1).min(axis=1)\n",
    "  next_q_value = Tensor(r) + GAMMA * next_value * Tensor(mask)\n",
    "  next_q_value = next_q_value.reshape(-1, 1)\n",
    "\n",
    "  # update the Q functions\n",
    "  Tensor.no_grad = False\n",
    "  q1_t, q2_t = q1(sa), q2(sa)\n",
    "  q1_loss = ((q1_t - next_q_value)**2).mean()\n",
    "  q2_loss = ((q2_t - next_q_value)**2).mean()\n",
    "  q_loss = q1_loss + q2_loss\n",
    "  q_opt.zero_grad()\n",
    "  q_loss.backward()\n",
    "  q_opt.step()\n",
    "  Tensor.no_grad = True\n",
    "  \n",
    "  # update soft params\n",
    "  for x,y in zip(hard_params, soft_params): y.assign((1-TAU)*y + TAU*x.detach()).realize()\n",
    "  \n",
    "  return q_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VPG with entropy\n",
    "\n",
    "# by 100 steps it starts to learn\n",
    "for episode in (t:=trange(300)):\n",
    "  # play an episode\n",
    "  obs, done = env.reset(), False\n",
    "  X,Y,rews = [], [], []\n",
    "  episode_steps = 0\n",
    "  while not done:\n",
    "    act_distribution = policy(Tensor(obs)).softmax().numpy()\n",
    "    ent = -sum(act_distribution * np.log2(act_distribution))\n",
    "    act = np.random.choice(2, p=act_distribution)\n",
    "    X.append(obs)\n",
    "    Y.append(act)\n",
    "    next_obs, rew, done, _ = env.step(act)\n",
    "    \n",
    "    episode_steps += 1\n",
    "    if done and episode_steps < env._max_episode_steps: rew = -10.0\n",
    "    rews.append(rew + ALPHA * ent)\n",
    "    \n",
    "    memory.append((obs, act, rew, done, next_obs))\n",
    "    obs = next_obs\n",
    "    #print(obs, rew, done)\n",
    "  \n",
    "  tmp = np.zeros((len(Y), 2), dtype=np.float32)\n",
    "  tmp[range(len(Y)), Y] = reward_to_go(rews)\n",
    "  \n",
    "  # learn the policy\n",
    "  Tensor.no_grad = False\n",
    "  policy_loss = -(policy(Tensor(X)).log_softmax() * Tensor(tmp)).mean()\n",
    "  policy_opt.zero_grad()\n",
    "  policy_loss.backward()\n",
    "  policy_opt.step()\n",
    "  Tensor.no_grad = True\n",
    "  \n",
    "  t.set_description(f\"{len(rews)} steps {policy_loss.numpy():.4f}\")\n",
    "  \n",
    "  #print(loss.numpy(), len(rews))\n",
    "  ep_lens.append(len(rews))\n",
    "  ep_rews.append(sum(rews))\n",
    "  ep_loss.append(policy_loss.numpy())\n",
    "plot(ep_lens)\n",
    "plot(ep_rews)\n",
    "figure()\n",
    "plot(ep_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e505b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn Q function (soft + double Q)\n",
    "# this is learned offline on the memory from the VPG training\n",
    "\n",
    "for i in trange(300):\n",
    "  idxs = [random.choice(len(memory)) for i in range(BS)]\n",
    "  samples = [memory[x] for x in idxs]\n",
    "  q_loss = learn_q_function(samples)\n",
    "  q_losses.append(q_loss)\n",
    "\n",
    "plot(q_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db07e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did the Q function learn?\n",
    "\n",
    "pts_x, pts_y = [], []\n",
    "rs, qs = [], []\n",
    "for s,a,r,done,ns in memory[::len(memory)//1000]:\n",
    "  pts_x.append(s[2])\n",
    "  pts_y.append(s[3])\n",
    "  rs.append(r)\n",
    "  # max of the Q function is the value, right?\n",
    "  qs.append(max(q1(Tensor(list(s)+[1,0])).numpy()[0], q1(Tensor(list(s)+[0,1])).numpy()[0]))\n",
    "plt.scatter(pts_x, pts_y, s=np.array(rs)*-1)\n",
    "figure()\n",
    "plt.scatter(pts_x, pts_y, s=np.array(qs)*-1, c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the functions look with numbers?\n",
    "\n",
    "def debug(s):\n",
    "  return q1(Tensor(s+[1,0])).numpy()[0], q1(Tensor(s+[0,1])).numpy()[0], policy(Tensor(s)).softmax().numpy()\n",
    "\n",
    "for x in np.arange(-0.6,0.7,0.1):\n",
    "  print(f\"{x:.2f}\",debug([0,0,x,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8fa162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cartpole, with either policy or Q function\n",
    "# both complete the full 500 steps (using argmax, not sampling)\n",
    "Tensor.no_grad = True\n",
    "obs, done = env.reset(), False\n",
    "act_policy = []\n",
    "act_q = []\n",
    "episode_steps = 0\n",
    "while not done:\n",
    "  # policy\n",
    "  act_distribution = policy(Tensor(obs)).numpy()\n",
    "  act_policy.append(act_distribution)\n",
    "  \n",
    "  # Q function\n",
    "  q_distribution = [min(q1(Tensor(list(obs)+[1,0])).numpy()[0], q2(Tensor(list(obs)+[1,0])).numpy()[0]), min(q1(Tensor(list(obs)+[0,1])).numpy()[0], q2(Tensor(list(obs)+[0,1])).numpy()[0])]\n",
    "  act_q.append(q_distribution)\n",
    "  \n",
    "  # use the policy or the Q function to act?\n",
    "  act = np.argmax(act_distribution)\n",
    "  #act = np.argmax(q_distribution)\n",
    "  \n",
    "  obs, rew, done, _ = env.step(act)\n",
    "  episode_steps += 1\n",
    "  env.render()\n",
    "plot(act_policy)\n",
    "plot(np.array(act_q)[:, 1] - np.array(act_q)[:, 0])\n",
    "figure()\n",
    "plot(act_q)\n",
    "episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c415d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft actor critic\n",
    "\n",
    "def update(learn_policy=False):\n",
    "  global memory\n",
    "  #memory = memory[-10000:]\n",
    "  idxs = [random.choice(len(memory)) for i in range(BS)]\n",
    "  samples = [memory[x] for x in idxs]\n",
    "  \n",
    "  # learn Q function\n",
    "  q_loss = learn_q_function(samples)\n",
    "  \n",
    "  if learn_policy:\n",
    "    # get states\n",
    "    s = Tensor(np.array([x[0] for x in samples]))\n",
    "\n",
    "    # compute the Q functions\n",
    "    # NOTE: we are not backpropping through them\n",
    "    act_0 = np.zeros((s.shape[0], 2), dtype=np.float32)\n",
    "    act_0[:, 0] = 1\n",
    "    act_1 = np.zeros((s.shape[0], 2), dtype=np.float32)\n",
    "    act_1[:, 1] = 1\n",
    "    spi0 = s.cat(Tensor(act_0), dim=1)\n",
    "    spi1 = s.cat(Tensor(act_1), dim=1)\n",
    "    min_qf_pi_0 = Tensor.cat(q1(spi0), q2(spi0), dim=1).min(axis=1, keepdim=True)\n",
    "    min_qf_pi_1 = Tensor.cat(q1(spi1), q2(spi1), dim=1).min(axis=1, keepdim=True)\n",
    "    min_qf_pi = min_qf_pi_0.cat(min_qf_pi_1, dim=1)\n",
    "\n",
    "    # run the policy\n",
    "    Tensor.no_grad = False\n",
    "    act_default_distribution = policy(s)\n",
    "    act_distribution = act_default_distribution.softmax()\n",
    "    act_log_distribution = (act_distribution + 1e-3).log()\n",
    "\n",
    "    # learn the policy\n",
    "    inside_term = ALPHA * act_log_distribution - min_qf_pi\n",
    "    policy_loss = (act_distribution * inside_term).sum(axis=1)\n",
    "    policy_loss = policy_loss.mean()\n",
    "    policy_opt.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_opt.step()\n",
    "    Tensor.no_grad = True\n",
    "    \n",
    "    policy_loss = policy_loss.numpy()\n",
    "  else:\n",
    "    policy_loss = None\n",
    "  \n",
    "  return q_loss, policy_loss\n",
    "  \n",
    "def run_episode():\n",
    "  Tensor.no_grad = True\n",
    "  done = False\n",
    "  state, done = env.reset(), False\n",
    "  episode_steps = 0\n",
    "  losses = None\n",
    "  while not done:\n",
    "    # act on policy\n",
    "    action, _ = policy.sample(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    episode_steps += 1\n",
    "    if done and episode_steps < env._max_episode_steps: reward = -10.0\n",
    "    memory.append((state, action, reward, done, next_state))\n",
    "    state = next_state\n",
    "  return episode_steps\n",
    "\n",
    "q_loss, policy_loss = np.nan, np.nan\n",
    "for i in (t:=trange(100)):\n",
    "  steps = run_episode()\n",
    "  if len(memory) >= BS:\n",
    "    update(False)\n",
    "    update(False)\n",
    "    llosses = update(True)\n",
    "    q_loss = llosses[0] if llosses[0] is not None else q_loss\n",
    "    policy_loss = llosses[1] if llosses[1] is not None else policy_loss\n",
    "  t.set_description(f\"{steps} steps {q_loss:.4f} {policy_loss:.4f}\")\n",
    "  env_steps.append(steps)\n",
    "  env_losses.append((q_loss, policy_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8307300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(env_steps)\n",
    "figure()\n",
    "plot([x[0] for x in env_losses])\n",
    "plot([x[1] for x in env_losses])\n",
    "plt.legend([\"q_loss\", \"policy_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385e8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
